##cluster1 lsb.queues
#Begin Queue
#QUEUE_NAME   = hybrid
#PRIORITY     = 30
#INTERACTIVE  = NO
#SNDJOBS_TO   = cloudq@cluster2
#FAIRSHARE    = USER_SHARES[[default,1]]
#DESCRIPTION  = Send jobs to remove cluster(s)
#HOSTS = none
#End Queue

Begin Queue
QUEUE_NAME   = ondemand
PRIORITY     = 30
INTERACTIVE  = NO
#RCVJOBS_FROM = cluster
FAIRSHARE    = USER_SHARES[[default,1]]
DESCRIPTION  = General queue
RC_HOSTS = all
RES_REQ = select[nodearray=='ondemand' && !cyclecloudmpi]
#RC_DEMAND_POLICY = 0
End Queue

Begin Queue
QUEUE_NAME   = ondemandmpi
PRIORITY     = 30
INTERACTIVE  = NO
#RCVJOBS_FROM = cluster
FAIRSHARE    = USER_SHARES[[default,1]]
DESCRIPTION  = Tightly coupled jobs
RC_HOSTS = all
RES_REQ = select[nodearray=='ondemandmpi' && cyclecloudmpi] same[placementgroup]
#RC_DEMAND_POLICY = 0
End Queue

Begin Queue
QUEUE_NAME   = lowprio
PRIORITY     = 30
INTERACTIVE  = NO
#RCVJOBS_FROM = cluster
FAIRSHARE    = USER_SHARES[[default,1]]
DESCRIPTION  = Preemptible jobs
RC_HOSTS = all
RES_REQ = select[nodearray=='lowprio' && !cyclecloudmpi]
End Queue

Begin Queue
QUEUE_NAME   = gpu
PRIORITY     = 30
INTERACTIVE  = NO
#RCVJOBS_FROM = cluster
FAIRSHARE    = USER_SHARES[[default,1]]
DESCRIPTION  = Preemptible jobs
RC_HOSTS = all
RES_REQ = select[nodearray=='gpu' && !cyclecloudmpi]
End Queue

Begin Queue
QUEUE_NAME   = gpumpi
PRIORITY     = 30
INTERACTIVE  = NO
#RCVJOBS_FROM = cluster
FAIRSHARE    = USER_SHARES[[default,1]]
DESCRIPTION  = Tightly coupled jobs
RC_HOSTS = all
RES_REQ = select[nodearray=='gpumpi' && cyclecloudmpi] same[placementgroup]
#RC_DEMAND_POLICY = 0
End Queue

Begin Queue
QUEUE_NAME   = manual
PRIORITY     = 30
INTERACTIVE  = NO
FAIRSHARE    = USER_SHARES[[default,1]]
DESCRIPTION  = Jobs on manually added nodes
RC_HOSTS = none
End Queue